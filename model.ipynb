{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61fec990-7828-4e47-955e-b68ca857e04d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbeafaf-5bb8-479e-b72c-1dbb5890b371",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff60767-58ab-45f9-b659-41197558ae79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# built-ins\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import traceback\n",
    "import time\n",
    "from os import path\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "\n",
    "# common\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# misc\n",
    "from IPython.display import display, clear_output, Markdown\n",
    "from termcolor import colored\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# metrics\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "# training\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fba2b0-4b6f-4b4d-b25f-95d4b6df3254",
   "metadata": {},
   "source": [
    "### Initial tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccde7130-45a6-4259-8830-0d0263e8c11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow multiple outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# suppress warnings\n",
    "import sys, os, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# also suppress warnings of parallel processes such as grid search cv\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\" # Also affect subprocesses\n",
    "    \n",
    "# configure pandas\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30543f6b-8ac9-4bfd-a49e-c46bf4c816b9",
   "metadata": {},
   "source": [
    "### Utils / Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f684f13-9007-45cc-a46f-82d9e1ff4593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enum(*sequential, **named):\n",
    "    enums = dict(zip(sequential, range(len(sequential))), **named)\n",
    "    return type('Enum', (), enums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7849014-1618-4956-a7f1-aa2730b85eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(a, b):\n",
    "    return {**a, **b}\n",
    "\n",
    "def cprint(text, color):\n",
    "    print(colored(text, color, attrs=['bold']))\n",
    "    \n",
    "def print_red(text):\n",
    "    cprint(text, 'red')\n",
    "\n",
    "def print_blue(text):\n",
    "    cprint(text, 'blue')\n",
    "    \n",
    "def print_dim(text):\n",
    "    print(colored(text, 'grey'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26175b8f-ae09-4444-9d39-d9e2a25e4113",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Time(object):\n",
    "    def __enter__(self):\n",
    "        self.start_time = time.time()\n",
    "  \n",
    "    def __exit__(self, *args, **kwargs):\n",
    "        print(\"--- took %.2f seconds ---\" % (time.time() - self.start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2f1417-840e-4608-9c16-e6edb7553317",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Output:\n",
    "    class printer(str):\n",
    "        def __repr__(self):\n",
    "            return self\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "    \n",
    "    def update(self, output):\n",
    "        output = self.printer(output)\n",
    "        \n",
    "        if self.out is None:\n",
    "            self.out = display(output, display_id=True)\n",
    "        else:\n",
    "            self.out.update(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f106a9-9b2f-4917-87d9-822a03307d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintDuration(object):\n",
    "    class printer(str):\n",
    "        def __repr__(self):\n",
    "            return self\n",
    "        \n",
    "    def __enter__(self):\n",
    "        self.start_time = datetime.now()\n",
    "        self.last_tick = self.start_time\n",
    "        self.tick_count = 0\n",
    "        self.tick_times = 0\n",
    "        \n",
    "        self.completed = False\n",
    "        self.progress = 0\n",
    "        self.ert = 0\n",
    "        self.att = 0\n",
    "        self.out = None\n",
    "        \n",
    "        return self.tick\n",
    "  \n",
    "    def __exit__(self, exc_type, exc_value, tb):\n",
    "        if exc_type is not None:\n",
    "            traceback.print_exception(exc_type, exc_value, tb)\n",
    "        \n",
    "        self.completed = True\n",
    "        self.render()\n",
    "        \n",
    "    def tdformat(self, seconds):\n",
    "        hours, remainder = divmod(seconds, 3600)\n",
    "        minutes, seconds = divmod(remainder, 60)\n",
    "        return '{:02}:{:02}:{:02}'.format(int(hours), int(minutes), int(seconds))\n",
    "    \n",
    "    def render(self):\n",
    "        output = ''\n",
    "        \n",
    "        if self.completed:\n",
    "            complete_time = (datetime.now() - self.start_time).total_seconds()\n",
    "            complete_time = self.tdformat(complete_time)\n",
    "            output = f'100% completed, total run time = {complete_time}'\n",
    "        else:\n",
    "            percent = round(self.progress * 100)\n",
    "            att = self.tdformat(self.att)\n",
    "            ert = self.tdformat(self.ert)\n",
    "            output = f'{percent}% completed, remaining time = {ert}, avg ticktime = {att}'\n",
    "        \n",
    "        output = self.printer(output)\n",
    "        \n",
    "        if self.out is None:\n",
    "            self.out = display(output, display_id=True)\n",
    "        else:\n",
    "            self.out.update(output)\n",
    "    \n",
    "    def tick(self, progress):\n",
    "        now = datetime.now()\n",
    "        \n",
    "        # calculate\n",
    "        work_time = (now - self.start_time).total_seconds()\n",
    "        tick_time = (now - self.last_tick).total_seconds()\n",
    "        self.tick_count += 1\n",
    "        self.tick_times += tick_time\n",
    "        avg_tick_time = self.tick_times // self.tick_count\n",
    "        \n",
    "        if progress > 0:\n",
    "            total_ticks = self.tick_count // progress\n",
    "            remained_ticks = total_ticks - self.tick_count\n",
    "            est_remain_time = avg_tick_time * remained_ticks\n",
    "        else:\n",
    "            est_remain_time = 0\n",
    "            \n",
    "        # set\n",
    "        self.progress = progress\n",
    "        self.att = avg_tick_time\n",
    "        self.ert = est_remain_time\n",
    "        \n",
    "        # render\n",
    "        self.render() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d04cc1-c4f0-4e7f-b701-9673b5599813",
   "metadata": {},
   "source": [
    "### Detect Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b86de6-5402-4905-9f14-8ca83226e44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_KAGGLE = os.environ.get('KAGGLE_KERNEL_RUN_TYPE') is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce4fc0d-3aff-4294-8df8-aa3a8980e199",
   "metadata": {},
   "source": [
    "### Path Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d230ae-2513-4cb8-aac0-e92acf95f37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_ROOT = '.'\n",
    "PATH_DATASET = path.join(PATH_ROOT, 'dataset')\n",
    "PATH_CSV = path.join(PATH_DATASET, 'csv')\n",
    "PATH_MODELS = path.join(PATH_ROOT, 'models')\n",
    "\n",
    "if ENV_KAGGLE:\n",
    "    PATH_ROOT = '/kaggle/working'\n",
    "    PATH_DATASET = '/kaggle/input/personal-key-indicators-of-heart-disease'\n",
    "    PATH_CSV = PATH_DATASET\n",
    "    PATH_MODELS = path.join(PATH_ROOT, 'models')\n",
    "    \n",
    "# Create directories.\n",
    "Path(PATH_MODELS).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bee298-3332-4e99-8213-4e114a3cda40",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047168fa-b476-44db-8628-364d814df20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COLUMN = 'HeartDisease'\n",
    "TARGET_CLASSES = ('No', 'Yes')\n",
    "TARGET = (TARGET_COLUMN, TARGET_CLASSES)\n",
    "\n",
    "PARAM_STRATEGY = enum('GRID_SEARCH', 'DEFAULTS', 'PREDEFINED')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd412b4-af60-4de8-bbe5-5d6d54fabb32",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8069bbe4-908e-4347-984d-f91785da4acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG_CSV_NAME = 'data.csv'\n",
    "\n",
    "if ENV_KAGGLE:\n",
    "    CFG_CSV_NAME = 'heart_2020_cleaned.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5cb0ce-1866-4eea-8af9-27b544fe93d1",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab744b3a-9a7f-44e8-a8e3-913a3e64e164",
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_SCORING = ('moved', 'f1', 1)\n",
    "HP_SEED = 339\n",
    "HP_CV_SPLITS = (10, 5)\n",
    "HP_TEST_SIZE = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e6f79a-5a50-487a-844b-e3571c0cd848",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3777b2-af4a-4803-8a53-0f147562ded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv\n",
    "data = pd.read_csv(path.join(PATH_CSV, CFG_CSV_NAME), encoding='utf-8')\n",
    "\n",
    "# drop duplicates\n",
    "data = data.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d721e4e0-a02c-40e6-9d0d-51b6c87f47d5",
   "metadata": {},
   "source": [
    "### Limit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1442f903-76db-4651-a66c-a7228e67715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit data helper\n",
    "def use_limited_data(data, limit):\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=limit, random_state=HP_SEED)\n",
    "    _, test_index = next(iter(splitter.split(data, data[TARGET_COLUMN])))\n",
    "    return data.loc[test_index].reset_index(drop=True)\n",
    "\n",
    "# always keep full data\n",
    "full_data = data\n",
    "\n",
    "# use limited data (for local tests)\n",
    "# data = use_limited_data(full_data, round(len(full_data) * 0.1)) # percent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b61b15a-a819-4ace-b7cf-5ca03d9c466f",
   "metadata": {},
   "source": [
    "### Simple EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fa4f1b-4481-4ce8-9ddb-709c84583a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457abc1b-aed7-4cc2-bf9d-ad7a5ce39046",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0075fbe-9745-4e3b-8d45-783a12f5a031",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7353a038-e5e1-46a8-878b-e25633ead766",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b79686-2d4e-4318-9f8e-e22722ad7d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb18852-09e1-4ee4-abf9-274c179f21e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = data.select_dtypes(exclude=['float64']).columns\n",
    "\n",
    "for col in categorical_columns:\n",
    "    col = data[col]\n",
    "    pd.concat([col.value_counts(normalize=True), col.value_counts()], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c51cac-4c2e-4958-81db-e47f96da4d91",
   "metadata": {},
   "source": [
    "# Pipeline Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f47915-f526-4288-9e95-adbc5b995612",
   "metadata": {},
   "source": [
    "## Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7565a8-d55f-4b27-8048-92cb456128ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(N, data, target, classes, indexes):\n",
    "    N = list(N)\n",
    "\n",
    "    # separate data\n",
    "    test_index, train_index = indexes\n",
    "    test = data.loc[test_index]\n",
    "    train = data.loc[train_index]\n",
    "\n",
    "    # calculate sizes\n",
    "    sizes = [sum(train[target] == c) for c in classes] \n",
    "\n",
    "    # fix sizes\n",
    "    for i, n in enumerate(N):\n",
    "        if type(n) is float:\n",
    "            N[i] = round(N[i] * sizes[i])\n",
    "\n",
    "    # do sampling\n",
    "    groups = [train[train[target] == c] for c in classes] \n",
    "    samples = [group.sample(N[i], replace=(sizes[i] < N[i])) for i, group in enumerate(groups)]\n",
    "    \n",
    "    # calculate final sizes\n",
    "    train_size = sum(N)\n",
    "    test_size = len(test_index)\n",
    "    \n",
    "    # calculate next indexes\n",
    "    train_index = list(range(train_size))\n",
    "    test_index = list(range(train_size, train_size + test_size))\n",
    "    \n",
    "    # create shuffled train dataframe\n",
    "    samples = pd.concat(samples, axis=0).sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # concatenate data\n",
    "    data = pd.concat([samples, test], axis=0).reset_index(drop=True)\n",
    "    \n",
    "    # return data and indexes\n",
    "    return data, test_index, train_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db47c8d-c986-4e46-b734-47d60aeaab6c",
   "metadata": {},
   "source": [
    "## Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b5ba61-c8a4-4698-9973-f10aa032ed87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlierRemover:\n",
    "    @staticmethod\n",
    "    def numeric(data):\n",
    "        cols = data.select_dtypes(include=['float64', 'int64']).columns.to_list()\n",
    "        return OutlierRemover(cols)\n",
    "    \n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "        self.bands = {}\n",
    "    \n",
    "    def fit(self, data):\n",
    "        for col in self.cols:\n",
    "            Q1 = data[col].quantile(0.25)\n",
    "            Q3 = data[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_band = Q1 - 1.5 * IQR\n",
    "            upper_band = Q3 + 1.5 * IQR\n",
    "            \n",
    "            self.bands[col] = (lower_band, upper_band)\n",
    "    \n",
    "    def transform(self, data):\n",
    "        for col in self.cols:\n",
    "            lower_band, upper_band = self.bands[col]\n",
    "            inliers = ~((data[col] < lower_band) | (data[col] > upper_band))\n",
    "            data = data[inliers]\n",
    "            \n",
    "        return data\n",
    "            \n",
    "    def fit_transform(self, data):\n",
    "        self.fit(data)\n",
    "        return self.transform(data)\n",
    "    \n",
    "class MultiLabelEncoder():\n",
    "    @staticmethod\n",
    "    def binary(data):\n",
    "        cols = [col for col in data.columns if data[col].nunique() == 2]\n",
    "        return MultiLabelEncoder(cols)\n",
    "    \n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "        self.encoders = {col: LabelEncoder() for col in cols}\n",
    "    \n",
    "    def fit(self, data):\n",
    "        for col in self.cols:\n",
    "            self.encoders[col].fit(data[col])\n",
    "\n",
    "    def transform(self, data):\n",
    "        for col in self.cols:\n",
    "            data[col] = self.encoders[col].transform(data[col])\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def fit_transform(self, data):\n",
    "        self.fit(data)\n",
    "        return self.transform(data)\n",
    "\n",
    "class Preprocessor:\n",
    "    @staticmethod\n",
    "    def params(override={}):\n",
    "        defaults = {\n",
    "            'target': 'HeartDisease',\n",
    "            'outlier_strategy': 'all',\n",
    "            'encode_labels': True,\n",
    "            'pca': False,\n",
    "            'onehot_encoding': ['Race', 'Diabetic'],\n",
    "            'ordinal_encoding': {\n",
    "                'GenHealth': ['Poor', 'Fair', 'Good', 'Very good','Excellent'],\n",
    "                'AgeCategory': ['18-24', '25-29','30-34', '35-39', '40-44', '45-49', '50-54',\n",
    "                                '55-59', '60-64', '65-69', '70-74', '75-79', '80 or older']\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return merge(defaults, override)\n",
    "    \n",
    "    @staticmethod\n",
    "    def process(*args, **kwargs):\n",
    "        processor = Preprocessor(*args, **kwargs)\n",
    "        processor.apply()\n",
    "        return processor\n",
    "    \n",
    "    def __init__(self, data, test_index=None, train_index=None, options=None):\n",
    "        if options is None:\n",
    "            options = Preprocessor.params()\n",
    "        \n",
    "        if train_index is None:\n",
    "            train_index = list(range(len(data)))\n",
    "            test_index = []\n",
    "        \n",
    "        self.data = data\n",
    "        self.test_index = np.array(test_index)\n",
    "        self.train_index = np.array(train_index)\n",
    "        self.options = options\n",
    "        \n",
    "        self.target = self.options['target']\n",
    "        self.update_meta()\n",
    "    \n",
    "    def update_meta(self):\n",
    "        self.features_mask = self.data.columns != self.target\n",
    "        self.columns = self.data.columns\n",
    "        self.feature_columns = self.columns[self.features_mask]\n",
    "        self.has_train = self.train_index.shape[0] > 0\n",
    "        self.has_test = self.test_index.shape[0] > 0\n",
    "\n",
    "    def override_data(self, value):\n",
    "        labels = self.y\n",
    "        self.data = pd.DataFrame(data=value, index=self.data.index)\n",
    "        self.data[self.target] = labels\n",
    "        self.update_meta()\n",
    "            \n",
    "    def get_x(self, df):\n",
    "        return df.drop(self.target, axis=1).to_numpy()\n",
    "    \n",
    "    def get_y(self, df):\n",
    "        return df[self.target].to_numpy()\n",
    "    \n",
    "    # .x getter/setter\n",
    "    @property\n",
    "    def x(self):\n",
    "        return self.get_x(self.data)\n",
    "    \n",
    "    @x.setter\n",
    "    def x(self, value):\n",
    "        self.data.loc[:, self.features_mask] = value\n",
    "        \n",
    "    # .y getter/setter\n",
    "    @property\n",
    "    def y(self):\n",
    "        return self.get_y(self.data)\n",
    "    \n",
    "    @y.setter\n",
    "    def y(self, value):\n",
    "        self.data.loc[:, [self.target]] = value\n",
    "    \n",
    "    # .test getter/setter\n",
    "    @property\n",
    "    def test(self):\n",
    "        return self.data.loc[self.test_index]\n",
    "    \n",
    "    @test.setter\n",
    "    def test(self, value):\n",
    "        self.data.loc[self.test_index] = value\n",
    "    \n",
    "    # .train getter/setter\n",
    "    @property\n",
    "    def train(self):\n",
    "        return self.data.loc[self.train_index]\n",
    "    \n",
    "    @train.setter\n",
    "    def train(self, value):\n",
    "        self.data.loc[self.train_index] = value\n",
    "    \n",
    "    # .x_test getter/setter\n",
    "    @property\n",
    "    def x_test(self):\n",
    "        return self.get_x(self.test)\n",
    "    \n",
    "    @x_test.setter\n",
    "    def x_test(self, value):\n",
    "        self.data.loc[self.test_index, self.features_mask] = value\n",
    "        \n",
    "    # .x_train getter/setter\n",
    "    @property\n",
    "    def x_train(self):\n",
    "        return self.get_x(self.train)\n",
    "    \n",
    "    @x_train.setter\n",
    "    def x_train(self, value):\n",
    "        self.data.loc[self.train_index, self.features_mask] = value\n",
    "    \n",
    "    # .y_test getter\n",
    "    @property\n",
    "    def y_test(self):\n",
    "        return self.get_y(self.test)\n",
    "        \n",
    "    # .y_test setter\n",
    "    @property\n",
    "    def y_train(self):\n",
    "        return self.get_y(self.train)\n",
    "    \n",
    "    def chop(self):\n",
    "        return self.x_test, self.y_test, self.x_train, self.y_train\n",
    "    \n",
    "    def apply(self):\n",
    "        # remove outliers\n",
    "        outlier_strategy = self.options.get('outlier_strategy', 'train_only')\n",
    "        outlier_remover = OutlierRemover.numeric(self.data)\n",
    "        if outlier_strategy == 'train_only':\n",
    "            self.train = outlier_remover.fit_transform(self.train)\n",
    "        elif outlier_strategy == 'include_test':\n",
    "            outlier_remover.fit(self.train())\n",
    "            self.data = outlier_remover.transform(self.data)\n",
    "        elif outlier_strategy == 'all':\n",
    "            self.data = outlier_remover.fit_transform(self.data)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        # update removed indexes.\n",
    "        indexes = self.data.index.values\n",
    "        self.train_index = self.train_index[np.isin(self.train_index, indexes)]\n",
    "        self.test_index = self.test_index[np.isin(self.test_index, indexes)]\n",
    "        \n",
    "        # encode labels\n",
    "        encode_labels = self.options.get('encode_labels', True)\n",
    "        if encode_labels:\n",
    "            self.data = MultiLabelEncoder.binary(self.data).fit_transform(self.data)\n",
    "            \n",
    "        onehot_encoding = self.options.get('onehot_encoding', None)\n",
    "        if onehot_encoding is not None:\n",
    "            cols = onehot_encoding\n",
    "            self.data = pd.get_dummies(self.data, columns=cols, prefix=cols)\n",
    "            self.update_meta()\n",
    "            \n",
    "        # ordinal encoding\n",
    "        ordinal_encoding = self.options.get('ordinal_encoding', None)\n",
    "        if ordinal_encoding is not None:\n",
    "            for col, ordinals in ordinal_encoding.items():\n",
    "                encoder = OrdinalEncoder(categories=[ordinals])\n",
    "                self.data[[col]] = encoder.fit_transform(self.data[[col]])\n",
    "        \n",
    "        # scaler\n",
    "        scale = self.options.get('scale', True)\n",
    "        if scale:\n",
    "            scaler = StandardScaler()\n",
    "            self.x_train = scaler.fit_transform(self.x_train)\n",
    "            \n",
    "            if self.has_test:\n",
    "                self.x_test = scaler.transform(self.x_test)\n",
    "                \n",
    "        # PCA\n",
    "        pca_n = self.options.get('pca', False)\n",
    "        if pca_n:\n",
    "            pca = PCA(n_components=pca_n, svd_solver='full', copy=True)\n",
    "            pca.fit(self.x_train)\n",
    "            result = pca.transform(self.x)\n",
    "            self.override_data(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef423cf-d613-4050-bfb2-b9ea4989afdc",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa74c236-cab3-4c93-93b8-222d7650a27b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, estimator, data, target, scoring, n_splits, test_size, seed,\n",
    "                 sampling=None, prep_params=None, hp_grid=None):\n",
    "        \n",
    "        self.estimator = estimator\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.scoring = scoring\n",
    "        self.n_splits = n_splits\n",
    "        self.test_size = test_size\n",
    "        self.seed = seed\n",
    "        self.sampling = sampling\n",
    "        self.prep_params = prep_params\n",
    "        self.hp_grid = hp_grid\n",
    "\n",
    "        self.trained = False\n",
    "        self.results = []\n",
    "        self.best_result = None\n",
    "        \n",
    "        self.parse_args()\n",
    "    \n",
    "    def parse_args(self):\n",
    "        self.n_test_split, self.n_grid_split = self.n_splits\n",
    "        \n",
    "        self.score_type, self.score_fn, self.score_cls = self.scoring\n",
    "        \n",
    "        self.target_column, self.target_classes = self.target\n",
    "        self.target_encoded_classes = list(range(len(self.target_classes)))\n",
    "    \n",
    "    @property\n",
    "    def model(self):\n",
    "        return self.best_result['model']\n",
    "    \n",
    "    @property\n",
    "    def preprocessor(self):\n",
    "        return self.best_result['preprocessor']\n",
    "    \n",
    "    @property\n",
    "    def scores(self):\n",
    "        return self.best_result['scores']\n",
    "    \n",
    "    @property\n",
    "    def score(self):\n",
    "        return self.scores[self.score_type][self.score_fn]\n",
    "    \n",
    "    def split(self):\n",
    "        split = StratifiedShuffleSplit(n_splits=self.n_test_split, test_size=self.test_size, random_state=self.seed)\n",
    "        return split.split(self.data, self.data[self.target_column])\n",
    "    \n",
    "    def train(self, tick=None):\n",
    "        for split_index, (train_index, test_index) in enumerate(self.split()):\n",
    "            if tick is not None:\n",
    "                tick(split_index/self.n_test_split)\n",
    "            \n",
    "            # get values\n",
    "            data = self.data\n",
    "            sampling = self.sampling\n",
    "            target_column = self.target_column\n",
    "            target_classes = self.target_classes\n",
    "            \n",
    "            # default values\n",
    "            best_params = None\n",
    "            best_estimator = None\n",
    "            \n",
    "            # do sampling\n",
    "            if sampling is not None:\n",
    "                # set sample sizes\n",
    "                sample_sizes = sampling\n",
    "\n",
    "                # if function\n",
    "                if callable(sampling):\n",
    "                    train_data = data.loc[train_index]\n",
    "                    sample_sizes = sampling(train_data)\n",
    "\n",
    "                # sample data\n",
    "                data, test_index, train_index = sample(sample_sizes, data, target_column, target_classes, (test_index, train_index))\n",
    "            \n",
    "            # preprocess data\n",
    "            preprocessor = Preprocessor.process(data, test_index, train_index, self.prep_params)\n",
    "            X_test, Y_test, X_train, Y_train = preprocessor.chop()\n",
    "                        \n",
    "            # fix grid\n",
    "            is_grid, hp_grid = self.fix_hp_grid(self.hp_grid)\n",
    "            \n",
    "            # if given parameters are grid do grid search\n",
    "            if is_grid:\n",
    "                # create grid searcher\n",
    "                gscv = GridSearchCV(estimator=self.estimator(), param_grid=hp_grid,\n",
    "                                  cv=self.n_grid_split, scoring=self.score_fn, n_jobs=-1)\n",
    "                \n",
    "                # fit\n",
    "                gscv.fit(X_train, Y_train)\n",
    "                \n",
    "                # collect best results\n",
    "                best_params = gscv.best_params_\n",
    "                best_estimator = gscv.best_estimator_\n",
    "            \n",
    "            # if given parameters are singular or none do direct training.\n",
    "            else:\n",
    "                # create and fit estimator\n",
    "                best_estimator = self.estimator(**hp_grid)\n",
    "                best_estimator.fit(X_train, Y_train)\n",
    "                best_params = hp_grid\n",
    "            \n",
    "            # get predictions\n",
    "            Y_pred = best_estimator.predict(X_test)\n",
    "            Y_prob = best_estimator.predict_proba(X_test)\n",
    "            \n",
    "            # create result \n",
    "            self.results.append({\n",
    "                'y_true': Y_test, 'y_pred': Y_pred, 'y_prob': Y_prob, \n",
    "                'params': best_params, 'model': best_estimator,\n",
    "                'preprocessor': preprocessor, 'seed': self.seed,\n",
    "            })\n",
    "            \n",
    "        self._calculate_scores()\n",
    "        self._set_best_result()\n",
    "        self.trained = True\n",
    "    \n",
    "    def fix_hp_grid(self, hp_grid=None):\n",
    "        if hp_grid is None:\n",
    "            return False, {}\n",
    "\n",
    "        # check if there is multidimensional value.\n",
    "        is_grid = sum([np.ndim(v) for v in hp_grid.values()]) > 0\n",
    "        \n",
    "        # fix singular values if suppose to be a grid.\n",
    "        if is_grid:\n",
    "            hp_grid = {k: [v] if np.ndim(v) == 0 else v for k, v in hp_grid.items()}\n",
    "\n",
    "        return is_grid, hp_grid\n",
    "        \n",
    "    def _calculate_scores(self):\n",
    "        classes = self.target_encoded_classes[::-1]\n",
    "\n",
    "        score_fns = {\n",
    "            'accuracy': accuracy_score,\n",
    "            'f1_micro': partial(f1_score, average='micro'),\n",
    "        }\n",
    "\n",
    "        metrics = {\n",
    "            'f1': f1_score,\n",
    "            'recall': recall_score,\n",
    "            'precision': precision_score,\n",
    "        }\n",
    "\n",
    "        for cls in classes:\n",
    "            for metric, fn in metrics.items():\n",
    "                name = metric if cls == self.score_cls else f'{cls}.{metric}'\n",
    "                score_fns[name] = partial(fn, pos_label=cls)\n",
    "\n",
    "        # calculate for each result.\n",
    "        for result in self.results:\n",
    "            # get targets\n",
    "            Y_true, Y_pred, Y_prob = result['y_true'], result['y_pred'], result['y_prob']\n",
    "\n",
    "            # create scores\n",
    "            scores = result['scores'] = dict(raw={}, moved={})\n",
    "\n",
    "            # calculate raw scores\n",
    "            for name, fn in score_fns.items():\n",
    "                scores['raw'][name] = fn(Y_true, Y_pred)\n",
    "\n",
    "            # calculate threshold moved scores\n",
    "            moved_score = -1\n",
    "            moved_pred = None\n",
    "            moved_threshold = None\n",
    "\n",
    "            # find the best threshold\n",
    "            for threshold in np.arange(0.5, 0, -0.01):\n",
    "                pred = (Y_prob[:, 1] >= threshold).astype(int)\n",
    "                score = score_fns[self.score_fn](Y_true, pred)\n",
    "\n",
    "                if score > moved_score:\n",
    "                    moved_pred = pred\n",
    "                    moved_score = score\n",
    "                    moved_threshold = threshold\n",
    "\n",
    "            # calculate threshold moved scores\n",
    "            for name, fn in score_fns.items():\n",
    "                scores['moved'][name] = fn(Y_true, moved_pred)\n",
    "\n",
    "            # keep the threshold info\n",
    "            result['moved_threshold'] = moved_threshold\n",
    "    \n",
    "    def _set_best_result(self):\n",
    "        best_score = -1\n",
    "        best_result = None\n",
    "        \n",
    "        for result in self.results:\n",
    "            score = result['scores'][self.score_type][self.score_fn]\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_result = result\n",
    "        \n",
    "        self.best_result = best_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318dd332-f392-4d7c-89fe-a897e451b25a",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9afae4-9888-4332-b759-b7a057025508",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Trainer:   \n",
    "    def __init__(self, name, data, target, scoring, n_splits, test_size, seed, sampling=None, prep_params=None):\n",
    "        self.name = name\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.scoring = scoring\n",
    "        self.n_splits = n_splits\n",
    "        self.test_size = test_size\n",
    "        self.seed = seed\n",
    "        self.sampling = sampling\n",
    "        self.prep_params = prep_params\n",
    "        \n",
    "        self.estimators = {}\n",
    "        self.param_strategy = PARAM_STRATEGY.DEFAULTS\n",
    "        self.predefined_params = {}\n",
    "    \n",
    "    def add_estimator(self, name, estimator, hp_grid=None):\n",
    "        self.estimators[name] = (name, estimator, hp_grid)\n",
    "    \n",
    "    def set_predefined_params(self, params):\n",
    "        self.predefined_params = params\n",
    "    \n",
    "    def use_param_strategy(self, strategy):\n",
    "        self.param_strategy = strategy\n",
    "    \n",
    "    def get_estimator_params(self, name):\n",
    "        if self.param_strategy == PARAM_STRATEGY.DEFAULTS:\n",
    "            return {}\n",
    "        \n",
    "        if self.param_strategy == PARAM_STRATEGY.PREDEFINED:\n",
    "            return self.predefined_params.get(name, {})\n",
    "        \n",
    "        if self.param_strategy == PARAM_STRATEGY.GRID_SEARCH:\n",
    "            return self.estimators[name][2]\n",
    "    \n",
    "    def get_model_path(self, name):\n",
    "        return path.join(PATH_MODELS, f'{self.name}_{name}.pickle')\n",
    "    \n",
    "    def save_model(self, name, model):\n",
    "        model_path = self.get_model_path(name)\n",
    "        with open(model_path,'wb') as file:\n",
    "            pickle.dump(model, file)\n",
    "        \n",
    "    def load_model(self, name):\n",
    "        model_path = self.get_model_path(name)\n",
    "        with open(model_path, 'rb') as file:\n",
    "            return pickle.load(file)\n",
    "        \n",
    "    def train_estimators(self, **kwargs):\n",
    "        estimators = kwargs.pop('estimators', self.estimators.keys())\n",
    "        for name in estimators:\n",
    "            print_red(f'Estimator: {name}\\n')\n",
    "            model = self.train_estimator(name, **kwargs)\n",
    "            yield (name, model)\n",
    "            \n",
    "    def train_estimator(self, name, reset=False, seed=None, save=True, print_duration=True):      \n",
    "        if seed is None:\n",
    "            seed = self.seed\n",
    "        \n",
    "        if not reset:\n",
    "            try:\n",
    "                model = self.load_model(name)\n",
    "                setattr(self, name, model)\n",
    "                \n",
    "                print(f'Model {name} is loaded from disk successfully.')\n",
    "                return model\n",
    "            \n",
    "            except:\n",
    "                model = None\n",
    "        \n",
    "        name, estimator, _ = self.estimators[name]\n",
    "        params = self.get_estimator_params(name)\n",
    "\n",
    "        model = Model(estimator, self.data, self.target, self.scoring, self.n_splits,\n",
    "                    self.test_size, seed, self.sampling, self.prep_params, params)\n",
    "        \n",
    "        if print_duration:\n",
    "            with PrintDuration() as tick:\n",
    "                model.train(tick)\n",
    "        else:\n",
    "            model.train()\n",
    "        \n",
    "        setattr(self, name, model)\n",
    "        if save:\n",
    "            self.save_model(name, model)\n",
    "            \n",
    "        return model\n",
    "    \n",
    "    def search_best_seed(self, name, seed_range=range(100)):\n",
    "        output = Output()\n",
    "        \n",
    "        best_score = -1\n",
    "        best_seed = 0\n",
    "        \n",
    "        print(f'Searching best seed for {name}')\n",
    "        \n",
    "        for seed in seed_range:\n",
    "            output.update(f'  -> Testing seed {seed}')\n",
    "            model = self.train_estimator(name=name, seed=seed, save=False, print_duration=False)\n",
    "            score = model.score\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_seed = seed\n",
    "                print(f'* {seed} -> {score}')\n",
    "        \n",
    "        print(f'Best seed found as {best_seed}')\n",
    "        return best_seed\n",
    "\n",
    "    def get_results_df(self, name, shuffle=False, ascending=False):\n",
    "        model = getattr(self, name)\n",
    "\n",
    "        true = model.best_result['y_true'].reshape(-1)\n",
    "        pred = model.best_result['y_pred'].reshape(-1)\n",
    "        \n",
    "        df = pd.DataFrame(data={\n",
    "            'true': true,\n",
    "            'prediction': pred,\n",
    "            'diff': np.absolute(true - pred)\n",
    "        })\n",
    "    \n",
    "        if shuffle:\n",
    "            df = df.sample(frac=1)\n",
    "        else:\n",
    "            df = df.sort_values('diff', ascending=ascending)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_scores_df(self, name):\n",
    "        model = getattr(self, name)\n",
    "        result = model.best_result\n",
    "\n",
    "        scores = model.scores\n",
    "        index = list(scores.keys())\n",
    "        cols = list(scores[index[0]].keys())\n",
    "        values = [[val for val in vals.values()] for vals in scores.values()]\n",
    "\n",
    "        return pd.DataFrame(values, index, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37acdacf-1446-4b4f-99a4-48755dff2da0",
   "metadata": {},
   "source": [
    "## SetTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad29e17-8d15-479f-ac58-f0280a4dc83f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SetTrainer:\n",
    "    def __init__(self):\n",
    "        self.estimators = {}\n",
    "        self.trainer_names = []\n",
    "        self.param_strategy = PARAM_STRATEGY.GRID_SEARCH\n",
    "        \n",
    "    def add_estimator(self, name, estimator, hp_grid=None):\n",
    "        self.estimators[name] = (name, estimator, hp_grid)\n",
    "    \n",
    "    def add_trainer(self, **kwargs):\n",
    "        name = kwargs['name']\n",
    "        trainer = Trainer(**kwargs)\n",
    "        \n",
    "        for estimator_name in self.estimators:\n",
    "            _, estimator, hp_grid = self.estimators[estimator_name]\n",
    "            hp_grid = deepcopy(hp_grid)\n",
    "            trainer.add_estimator(estimator_name, estimator, hp_grid)\n",
    "        \n",
    "        trainer.use_param_strategy(self.param_strategy)\n",
    "        \n",
    "        self.trainer_names.append(name)\n",
    "        setattr(self, name, trainer)\n",
    "    \n",
    "    def use_param_strategy(self, strategy):\n",
    "        self.param_strategy = strategy\n",
    "    \n",
    "    def run_trainer(self, name, **kwargs):\n",
    "        trainer = getattr(self, name)\n",
    "        for (model_name, model) in trainer.train_estimators(**kwargs):\n",
    "            yield (name, trainer, model_name, model)\n",
    "            \n",
    "    def run_all_trainers(self, **kwargs):\n",
    "        trainers = kwargs.pop('trainers', self.trainer_names)\n",
    "        count = len(trainers)\n",
    "        \n",
    "        for index, name in enumerate(trainers):\n",
    "            print_blue(f'Trainer {index+1}/{count}: {name}\\n')\n",
    "            for (trainer_name, trainer, model_name, model) in self.run_trainer(name, **kwargs):\n",
    "                yield (trainer_name, trainer, model_name, model)\n",
    "                \n",
    "    def save_scores(self):\n",
    "        columns = ['trainer', 'model']\n",
    "        values = []\n",
    "\n",
    "        score_cols = None\n",
    "\n",
    "        for trainer_name in self.trainer_names:\n",
    "            trainer = getattr(self, trainer_name)\n",
    "\n",
    "            for model_name in trainer.estimators:\n",
    "                model = getattr(trainer, model_name, None)\n",
    "\n",
    "                if model is not None and model.trained:\n",
    "                    value = [trainer_name, model_name[:10]]\n",
    "\n",
    "                    if score_cols is None:\n",
    "                        score_cols = [f'{group[:1]}/{fn[:]}' for group in model.scores for fn in model.scores[group]]\n",
    "\n",
    "                    value += [round(val, 3) for group in model.scores.values() for val in group.values()]\n",
    "                    values.append(value)\n",
    "\n",
    "        columns += score_cols\n",
    "\n",
    "        df = pd.DataFrame(values, columns=columns)\n",
    "        df.to_csv(path.join(PATH_MODELS, f'scores.csv'), index=False)\n",
    "        df.sort_values(by='model', inplace=True)\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c033179c-4f7a-449a-a301-5301cbe94531",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cac4266-3809-4fc6-9b64-b12e26d554c9",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e8d6c5-940b-432e-9f23-3fce89762671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_feature_importances(data, target, scoring, n_splits, test_size, seed):\n",
    "    model = Model(RandomForestClassifier, data, target, scoring, n_splits, test_size, seed)\n",
    "    model.train()\n",
    "    \n",
    "    importances = model.model.feature_importances_\n",
    "    indices = np.argsort(importances)\n",
    "    graph_x = range(len(indices))\n",
    "    \n",
    "    plt.title('Feature Importances')\n",
    "    plt.barh(graph_x, importances[indices], color='b', align='center')\n",
    "    plt.yticks(graph_x, model.preprocessor.feature_columns[indices])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.show()\n",
    "\n",
    "data_ = use_limited_data(full_data, 3200)\n",
    "calculate_feature_importances(data=data_, target=TARGET, scoring=HP_SCORING,\n",
    "                              n_splits=HP_CV_SPLITS, test_size=HP_TEST_SIZE, seed=HP_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4e4b8d-e880-48f1-85c8-c1c0b59ce133",
   "metadata": {},
   "source": [
    "### Explained Variance Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4f055e-c2b1-456b-9022-c1aea58d570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_explained_variance_ratio(data, p=0.95):\n",
    "    X = Preprocessor.process(data).data\n",
    "    \n",
    "    pca = PCA(n_components=None, svd_solver='full', copy=True)\n",
    "    reduced = pca.fit_transform(X)\n",
    "    \n",
    "    # extract the smallest number of components which\n",
    "    # explain at least p% (e.g. 80%) of the variance\n",
    "    n_components = 1 + np.argmax(np.cumsum(pca.explained_variance_ratio_) >= p)\n",
    "    print(f'For p={int(p*100)}% n_components should be {n_components}\\n')\n",
    "\n",
    "    # extract the values of the selected components\n",
    "    #Z = pca.transform(X)[:, :n_components]\n",
    "    \n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('number of components')\n",
    "    plt.ylabel('cumulative explained variance')\n",
    "    \n",
    "data_ = use_limited_data(full_data, 3200)\n",
    "calculate_explained_variance_ratio(data_, 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc31471-b278-48ad-8492-19f6d513fcb0",
   "metadata": {},
   "source": [
    "### Best Seed Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305f03ba-fdbd-45c9-a92d-3034cd1b151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_best_seed(seed_range, estimators, data, target, scoring, n_splits, test_size):\n",
    "    trainer = Trainer('trainer', data, target, scoring, n_splits, test_size, seed=0)\n",
    "\n",
    "    for estimator in estimators:\n",
    "        name = estimator.__name__\n",
    "        \n",
    "        trainer.add_estimator(name, estimator)\n",
    "        trainer.search_best_seed(name, seed_range)\n",
    "\n",
    "calculate_best_seed(seed_range=range(0), estimators=[LogisticRegression],\n",
    "                    data=full_data, target=TARGET, scoring=HP_SCORING,\n",
    "                    n_splits=(3, 1), test_size=HP_TEST_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6438daf-1ae4-42b0-8371-e7d412edb454",
   "metadata": {},
   "source": [
    "# Model Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d07addb-74e9-4016-b21e-8a27d2d69488",
   "metadata": {},
   "source": [
    "#### Parameter Strategies\n",
    "\n",
    "- `PARAM_STRATEGY.GRID_SEARCH` Will do grid search with provided hyperparameters grid above.\n",
    "- `PARAM_STRATEGY.DEFAULTS` Will use empty dict, {}, as parameters, which causes default parameters to be used.\n",
    "- `PARAM_STRATEGY.PREDEFINED` Will use predefined singular parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a34e912-c644-4998-b3bb-cb5197411128",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_trainer = SetTrainer()\n",
    "\n",
    "# set default trainer parameter strategy \n",
    "set_trainer.use_param_strategy(PARAM_STRATEGY.GRID_SEARCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844a9826-8b4f-4edc-b4da-c4d1f0111faa",
   "metadata": {},
   "source": [
    "## Logistic Regression \n",
    "[docs](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a942b4f2-3471-47b6-87f1-fff0c413676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add estimator\n",
    "set_trainer.add_estimator('logistic', LogisticRegression, {\n",
    "    'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "    'C': [0.8, 1.0, 1.2],\n",
    "    'max_iter': [50, 100, 200],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09531e2-69a2-4af0-80d3-17b2a7ae4e82",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes \n",
    "[docs](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html), [comparing naive bayes classification algorithms](https://towardsdatascience.com/comparing-a-variety-of-naive-bayes-classification-algorithms-fc5fa298379e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d378294b-f0d9-4d0b-90a2-1cc937165625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add estimator\n",
    "set_trainer.add_estimator('nb_gaussian', GaussianNB, {\n",
    "    'var_smoothing': [0, 1e-10, 1e-9, 1e-8]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aec9e2-11a5-4f6d-80de-3021230c6390",
   "metadata": {},
   "source": [
    "## kNN \n",
    "[docs](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a75ea74-2684-47b1-ad08-4e177ff9a31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add estimator\n",
    "set_trainer.add_estimator('knn', KNeighborsClassifier, {\n",
    "    'n_neighbors': [3, 5, 7],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'p': [1, 2],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8242ecbe-45b5-4b4e-8f69-ba3479fcaf81",
   "metadata": {},
   "source": [
    "## Decision Tree \n",
    "[docs](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b449782-e490-4fb9-99b9-067e775eecfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add estimator\n",
    "set_trainer.add_estimator('dt', DecisionTreeClassifier, {\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'min_samples_split': [1, 2, 4],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'max_features': [None, 0.8, 0.5],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbc7161-56a1-41ac-b392-6237082b3787",
   "metadata": {},
   "source": [
    "## MLP \n",
    "[docs](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b3f470-be66-4230-92b7-c9f69ee88be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add estimator\n",
    "set_trainer.add_estimator('mlp', MLPClassifier, {\n",
    "    'hidden_layer_sizes': [(100,), (128,128), (256,)],\n",
    "    'activation': ['relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    'learning_rate_init': [0.001],\n",
    "    'early_stopping': [True],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435288b3-f29b-437f-a930-03fb3c6d8ca8",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "[docs](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6add1530-4f25-4ab6-b149-0f331bc74f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add estimator\n",
    "set_trainer.add_estimator('bag', BaggingClassifier, {\n",
    "    'n_estimators': [5, 10, 20],\n",
    "    'max_samples': [0.7, 1.0],\n",
    "    'max_features': [0.7, 1.0],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dafccce-a01a-451f-81f1-69a5c45cbb63",
   "metadata": {},
   "source": [
    "## Random Forest \n",
    "[docs](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f14bec0-d396-42b0-99e8-3f45b0fbad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add estimator\n",
    "set_trainer.add_estimator('rf', RandomForestClassifier, {\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'min_samples_split': [2, 4],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': [\"sqrt\", 1.0, 0.7],\n",
    "    'max_samples': [None, 1.0, 0.7],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f6f809-7175-47ab-b085-abfa645b5455",
   "metadata": {},
   "source": [
    "## Gradient Boosting \n",
    "[docs](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0166a4d8-5182-4da3-a029-1e13115b106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add estimator\n",
    "set_trainer.add_estimator('gb', GradientBoostingClassifier, {\n",
    "    'min_samples_split': [2, 4],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_depth': [3, 5],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e1ef86-ceb8-48b4-8579-3baa672563ad",
   "metadata": {},
   "source": [
    "## AdaBoost \n",
    "[docs](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c54d9a5-7d34-4b75-b44d-751d67a8562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add estimator\n",
    "set_trainer.add_estimator('ada', LGBMClassifier, {\n",
    "    'n_estimators': [30, 50, 100],\n",
    "    'learning_rate': [0.8, 1.0, 1.2],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16836017-270d-4598-8779-a8d6fcc3dcb6",
   "metadata": {},
   "source": [
    "## LightGBM\n",
    "[docs](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c6722b-df47-45e6-a9a5-202b1b37f58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add estimator\n",
    "set_trainer.add_estimator('lgbm', LGBMClassifier, {\n",
    "    'boosting_type': ['gbdt', 'dart', 'goss', 'rf'],\n",
    "    'learning_rate': [0.05, 0.1, 0.15],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4100e5b2-fcb5-4165-b525-008fe573ebaf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7acadd-ec1d-404f-80d2-b3a6b87eb608",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2dbd63-6c74-48b0-a3a9-f77707d1fd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_sampling_size(train_data):\n",
    "    cls0_count = train_data[TARGET_COLUMN].value_counts()[TARGET_CLASSES[0]]\n",
    "    \n",
    "    cls0_factor = 0.5\n",
    "    cls1_factor = 0.4 * cls0_factor * cls0_count\n",
    "    \n",
    "    return (cls0_factor, round(cls1_factor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee1a284-405f-40cd-ac82-73eee63f737a",
   "metadata": {},
   "source": [
    "## Trainer Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36468257-ab77-4496-9b25-da7630dc57eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Default Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774b04ba-7138-4a79-8be0-db54c94c45e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate preprocessor parameters\n",
    "prep_params = Preprocessor.params({\n",
    "    'outlier_strategy': 'all',\n",
    "    'encode_labels': True,\n",
    "    'pca': False,\n",
    "})\n",
    "\n",
    "# set sampling size\n",
    "sampling_size = default_sampling_size\n",
    "\n",
    "# add trainer.\n",
    "set_trainer.add_trainer(name='default', data=data, target=TARGET, scoring=HP_SCORING,\n",
    "                        n_splits=HP_CV_SPLITS, test_size=HP_TEST_SIZE, seed=HP_SEED,\n",
    "                        sampling=sampling_size, prep_params=prep_params)\n",
    "\n",
    "# set predefined parameters\n",
    "# these parameters are found by grid-search with %10 of data.\n",
    "set_trainer.default.set_predefined_params({\n",
    "    'logistic': {'C': 1.0, 'max_iter': 50, 'penalty': 'l2'},\n",
    "    'nb_gaussian': {'var_smoothing': 0},\n",
    "    'knn': {'n_neighbors': 7, 'p': 2, 'weights': 'distance'},\n",
    "    'dt': {'criterion': 'entropy', 'max_features': 0.5, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'random'},\n",
    "    'mlp': {'activation': 'relu', 'early_stopping': True, 'hidden_layer_sizes': (128, 128), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'solver': 'adam'},\n",
    "    'bag': {'max_features': 0.7, 'max_samples': 1.0, 'n_estimators': 20},\n",
    "    'rf': {'criterion': 'gini', 'max_features': 'sqrt', 'max_samples': 1.0, 'min_samples_leaf': 1, 'min_samples_split': 2},\n",
    "    'gb': {'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 4},\n",
    "    'ada': {'learning_rate': 0.8, 'n_estimators': 100},\n",
    "    'lgbm': {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'learning_rate': 0.15, 'n_estimators': 200, 'subsample': 0.8},\n",
    "})\n",
    "\n",
    "# configure param strategy (otherwise uses set_trainer's default)\n",
    "set_trainer.default.use_param_strategy(PARAM_STRATEGY.PREDEFINED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d22dce5-7bc9-47ad-bdaf-0be55a99a9af",
   "metadata": {},
   "source": [
    "### PCA Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6788605-9a75-432a-ae71-ad47cac98d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate preprocessor parameters\n",
    "prep_params = Preprocessor.params({\n",
    "    'outlier_strategy': 'all',\n",
    "    'encode_labels': True,\n",
    "    'pca': 0.95,\n",
    "})\n",
    "\n",
    "# set sampling size\n",
    "sampling_size = default_sampling_size\n",
    "\n",
    "# add trainer.\n",
    "set_trainer.add_trainer(name='pca', data=data, target=TARGET, scoring=HP_SCORING,\n",
    "                        n_splits=HP_CV_SPLITS, test_size=HP_TEST_SIZE, seed=HP_SEED,\n",
    "                        sampling=sampling_size, prep_params=prep_params)\n",
    "\n",
    "# set predefined parameters\n",
    "# these parameters are found by grid-search with %10 of data.\n",
    "set_trainer.pca.set_predefined_params({\n",
    "    'logistic': {'C': 0.8, 'max_iter': 50, 'penalty': 'l2'},\n",
    "    'nb_gaussian': {'var_smoothing': 0},\n",
    "    'knn': {'n_neighbors': 7, 'p': 2, 'weights': 'distance'},\n",
    "    'dt': {'criterion': 'entropy', 'max_features': 0.8, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'best'},\n",
    "    'mlp': {'activation': 'relu', 'early_stopping': True, 'hidden_layer_sizes': (128, 128), 'learning_rate': 'adaptive', 'learning_rate_init': 0.001, 'solver': 'adam'},\n",
    "    'bag': {'max_features': 0.7, 'max_samples': 1.0, 'n_estimators': 20},\n",
    "    'rf': {'criterion': 'gini', 'max_features': 'sqrt', 'max_samples': 1.0, 'min_samples_leaf': 1, 'min_samples_split': 2},\n",
    "    'gb': {'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 2},\n",
    "    'ada': {'learning_rate': 0.8, 'n_estimators': 50},\n",
    "    'lgbm': {'boosting_type': 'gbdt', 'colsample_bytree': 1.0, 'learning_rate': 0.15, 'n_estimators': 200, 'subsample': 0.8},\n",
    "})\n",
    "\n",
    "# configure param strategy (otherwise uses set_trainer's default)\n",
    "set_trainer.pca.use_param_strategy(PARAM_STRATEGY.PREDEFINED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3779f6bb-495d-4274-87bb-44d4b560ef0c",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5c76bb-9150-422b-9870-1ea3d3c81f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training options\n",
    "options = {\n",
    "    # set trainer parameters\n",
    "    'trainers': ['default', 'pca'],\n",
    "    \n",
    "    # trainer parameters\n",
    "    'estimators': ['logistic', 'nb_gaussian', 'knn'],\n",
    "    #'reset': False,\n",
    "    #'seed': None,\n",
    "    #'save': False,\n",
    "}\n",
    "\n",
    "# train generator\n",
    "train_gen = set_trainer.run_all_trainers(**options)\n",
    "\n",
    "# train all \n",
    "for (trainer_name, trainer, model_name, model) in train_gen:\n",
    "    # save all scores\n",
    "    _ = set_trainer.save_scores()\n",
    "    \n",
    "    # Get best params if doing grid search.\n",
    "    if trainer.param_strategy == PARAM_STRATEGY.GRID_SEARCH:\n",
    "        print_dim(f\"\\nbest params: {model.best_result['params']}\")\n",
    "    \n",
    "    # Show stats.\n",
    "    print()\n",
    "    trainer.get_scores_df(model_name).head()\n",
    "    print()\n",
    "\n",
    "    # Show predicts.\n",
    "    #trainer.get_results_df(model_name, ascending=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833d1243-41ab-4089-9aa4-2902112dae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_trainer.save_scores()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
