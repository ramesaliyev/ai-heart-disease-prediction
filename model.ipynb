{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61fec990-7828-4e47-955e-b68ca857e04d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbeafaf-5bb8-479e-b72c-1dbb5890b371",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff60767-58ab-45f9-b659-41197558ae79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# built-ins\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import traceback\n",
    "import time\n",
    "from os import path\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "\n",
    "# common\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# misc\n",
    "from IPython.display import display, clear_output, Markdown\n",
    "from termcolor import colored\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# metrics\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "# training\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fba2b0-4b6f-4b4d-b25f-95d4b6df3254",
   "metadata": {},
   "source": [
    "### Initial tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccde7130-45a6-4259-8830-0d0263e8c11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow multiple outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# suppress warnings\n",
    "import sys, os, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# also suppress warnings of parallel processes such as grid search cv\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\" # Also affect subprocesses\n",
    "    \n",
    "# configure pandas\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30543f6b-8ac9-4bfd-a49e-c46bf4c816b9",
   "metadata": {},
   "source": [
    "### Utils / Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7849014-1618-4956-a7f1-aa2730b85eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(a, b):\n",
    "    return {**a, **b}\n",
    "\n",
    "def cprint(text, color):\n",
    "    print(colored(text, color, attrs=['bold']))\n",
    "    \n",
    "def print_red(text):\n",
    "    cprint(text, 'red')\n",
    "\n",
    "def print_blue(text):\n",
    "    cprint(text, 'blue')\n",
    "    \n",
    "def print_dim(text):\n",
    "    print(colored(text, 'grey'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26175b8f-ae09-4444-9d39-d9e2a25e4113",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Time(object):\n",
    "    def __enter__(self):\n",
    "        self.start_time = time.time()\n",
    "  \n",
    "    def __exit__(self, *args, **kwargs):\n",
    "        print(\"--- took %.2f seconds ---\" % (time.time() - self.start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2f1417-840e-4608-9c16-e6edb7553317",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Output:\n",
    "    class printer(str):\n",
    "        def __repr__(self):\n",
    "            return self\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "    \n",
    "    def update(self, output):\n",
    "        output = self.printer(output)\n",
    "        \n",
    "        if self.out is None:\n",
    "            self.out = display(output, display_id=True)\n",
    "        else:\n",
    "            self.out.update(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f106a9-9b2f-4917-87d9-822a03307d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintDuration(object):\n",
    "    class printer(str):\n",
    "        def __repr__(self):\n",
    "            return self\n",
    "        \n",
    "    def __enter__(self):\n",
    "        self.start_time = datetime.now()\n",
    "        self.last_tick = self.start_time\n",
    "        self.tick_count = 0\n",
    "        self.tick_times = 0\n",
    "        \n",
    "        self.completed = False\n",
    "        self.progress = 0\n",
    "        self.ert = 0\n",
    "        self.att = 0\n",
    "        self.out = None\n",
    "        \n",
    "        return self.tick\n",
    "  \n",
    "    def __exit__(self, exc_type, exc_value, tb):\n",
    "        if exc_type is not None:\n",
    "            traceback.print_exception(exc_type, exc_value, tb)\n",
    "        \n",
    "        self.completed = True\n",
    "        self.render()\n",
    "        \n",
    "    def tdformat(self, seconds):\n",
    "        hours, remainder = divmod(seconds, 3600)\n",
    "        minutes, seconds = divmod(remainder, 60)\n",
    "        return '{:02}:{:02}:{:02}'.format(int(hours), int(minutes), int(seconds))\n",
    "    \n",
    "    def render(self):\n",
    "        output = ''\n",
    "        \n",
    "        if self.completed:\n",
    "            complete_time = (datetime.now() - self.start_time).total_seconds()\n",
    "            complete_time = self.tdformat(complete_time)\n",
    "            output = f'100% completed, total run time = {complete_time}'\n",
    "        else:\n",
    "            percent = round(self.progress * 100)\n",
    "            att = self.tdformat(self.att)\n",
    "            ert = self.tdformat(self.ert)\n",
    "            output = f'{percent}% completed, remaining time = {ert}, avg ticktime = {att}'\n",
    "        \n",
    "        output = self.printer(output)\n",
    "        \n",
    "        if self.out is None:\n",
    "            self.out = display(output, display_id=True)\n",
    "        else:\n",
    "            self.out.update(output)\n",
    "    \n",
    "    def tick(self, progress):\n",
    "        now = datetime.now()\n",
    "        \n",
    "        # calculate\n",
    "        work_time = (now - self.start_time).total_seconds()\n",
    "        tick_time = (now - self.last_tick).total_seconds()\n",
    "        self.tick_count += 1\n",
    "        self.tick_times += tick_time\n",
    "        avg_tick_time = self.tick_times // self.tick_count\n",
    "        \n",
    "        if progress > 0:\n",
    "            total_ticks = self.tick_count // progress\n",
    "            remained_ticks = total_ticks - self.tick_count\n",
    "            est_remain_time = avg_tick_time * remained_ticks\n",
    "        else:\n",
    "            est_remain_time = 0\n",
    "            \n",
    "        # set\n",
    "        self.progress = progress\n",
    "        self.att = avg_tick_time\n",
    "        self.ert = est_remain_time\n",
    "        \n",
    "        # render\n",
    "        self.render() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d04cc1-c4f0-4e7f-b701-9673b5599813",
   "metadata": {},
   "source": [
    "### Detect Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b86de6-5402-4905-9f14-8ca83226e44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_KAGGLE = os.environ.get('KAGGLE_KERNEL_RUN_TYPE') is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce4fc0d-3aff-4294-8df8-aa3a8980e199",
   "metadata": {},
   "source": [
    "### Path Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d230ae-2513-4cb8-aac0-e92acf95f37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_root = '.'\n",
    "path_dataset = path.join(path_root, 'dataset')\n",
    "path_csv = path.join(path_dataset, 'csv')\n",
    "path_models = path.join(path_root, 'models')\n",
    "\n",
    "if ENV_KAGGLE:\n",
    "    path_root = '/kaggle/working'\n",
    "    path_dataset = '/kaggle/input/personal-key-indicators-of-heart-disease'\n",
    "    path_csv = path_dataset\n",
    "    path_models = path.join(path_root, 'models')\n",
    "    \n",
    "# Create directories.\n",
    "Path(path_models).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bee298-3332-4e99-8213-4e114a3cda40",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047168fa-b476-44db-8628-364d814df20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COLUMN = 'HeartDisease'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd412b4-af60-4de8-bbe5-5d6d54fabb32",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8069bbe4-908e-4347-984d-f91785da4acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_autosave_models = False\n",
    "cfg_force_train = False\n",
    "cfg_csv_name = 'data.csv'\n",
    "\n",
    "if ENV_KAGGLE:\n",
    "    cfg_autosave_models = True\n",
    "    cfg_force_train = False\n",
    "    cfg_csv_name = 'heart_2020_cleaned.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5cb0ce-1866-4eea-8af9-27b544fe93d1",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab744b3a-9a7f-44e8-a8e3-913a3e64e164",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_scoring = ('moved', 'f1')\n",
    "hp_seed = 0\n",
    "hp_cv_splits = (2, 2)\n",
    "hp_test_size = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e6f79a-5a50-487a-844b-e3571c0cd848",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3777b2-af4a-4803-8a53-0f147562ded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv\n",
    "data = pd.read_csv(path.join(path_csv, cfg_csv_name), encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d721e4e0-a02c-40e6-9d0d-51b6c87f47d5",
   "metadata": {},
   "source": [
    "## Limit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1442f903-76db-4651-a66c-a7228e67715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit data helper\n",
    "def use_limited_data(data, limit):\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=limit, random_state=hp_seed)\n",
    "    _, test_index = next(iter(splitter.split(data, data[TARGET_COLUMN])))\n",
    "    return data.loc[test_index].reset_index(drop=True)\n",
    "\n",
    "# always keep full data\n",
    "full_data = data\n",
    "\n",
    "# use limited data (for local tests)\n",
    "# data = use_limited_data(full_data, 3200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b61b15a-a819-4ace-b7cf-5ca03d9c466f",
   "metadata": {},
   "source": [
    "### Simple EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fa4f1b-4481-4ce8-9ddb-709c84583a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457abc1b-aed7-4cc2-bf9d-ad7a5ce39046",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0075fbe-9745-4e3b-8d45-783a12f5a031",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7353a038-e5e1-46a8-878b-e25633ead766",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b79686-2d4e-4318-9f8e-e22722ad7d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c51cac-4c2e-4958-81db-e47f96da4d91",
   "metadata": {},
   "source": [
    "# Pipeline Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b5ba61-c8a4-4698-9973-f10aa032ed87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlierRemover:\n",
    "    @staticmethod\n",
    "    def numeric(data):\n",
    "        cols = data.select_dtypes(include=['float64', 'int64']).columns.to_list()\n",
    "        return OutlierRemover(cols)\n",
    "    \n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "        self.bands = {}\n",
    "    \n",
    "    def fit(self, data):\n",
    "        for col in self.cols:\n",
    "            Q1 = data[col].quantile(0.25)\n",
    "            Q3 = data[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_band = Q1 - 1.5 * IQR\n",
    "            upper_band = Q3 + 1.5 * IQR\n",
    "            \n",
    "            self.bands[col] = (lower_band, upper_band)\n",
    "    \n",
    "    def transform(self, data):\n",
    "        for col in self.cols:\n",
    "            lower_band, upper_band = self.bands[col]\n",
    "            inliers = ~((data[col] < lower_band) | (data[col] > upper_band))\n",
    "            data = data[inliers]\n",
    "            \n",
    "        return data\n",
    "            \n",
    "    def fit_transform(self, data):\n",
    "        self.fit(data)\n",
    "        return self.transform(data)\n",
    "    \n",
    "class MultiLabelEncoder():\n",
    "    @staticmethod\n",
    "    def binary(data):\n",
    "        cols = [col for col in data.columns if data[col].nunique() == 2]\n",
    "        return MultiLabelEncoder(cols)\n",
    "    \n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "        self.encoders = {col: LabelEncoder() for col in cols}\n",
    "    \n",
    "    def fit(self, data):\n",
    "        for col in self.cols:\n",
    "            self.encoders[col].fit(data[col])\n",
    "\n",
    "    def transform(self, data):\n",
    "        for col in self.cols:\n",
    "            data[col] = self.encoders[col].transform(data[col])\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def fit_transform(self, data):\n",
    "        self.fit(data)\n",
    "        return self.transform(data)\n",
    "\n",
    "class Preprocessor:\n",
    "    @staticmethod\n",
    "    def params(override={}):\n",
    "        defaults = {\n",
    "            'target': 'HeartDisease',\n",
    "            'outlier_strategy': 'all',\n",
    "            'encode_labels': True,\n",
    "            'pca': False,\n",
    "            'onehot_encoding': ['Race', 'Diabetic'],\n",
    "            'ordinal_encoding': {\n",
    "                'GenHealth': ['Poor', 'Fair', 'Good', 'Very good','Excellent'],\n",
    "                'AgeCategory': ['18-24', '25-29','30-34', '35-39', '40-44', '45-49', '50-54',\n",
    "                                '55-59', '60-64', '65-69', '70-74', '75-79', '80 or older']\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return merge(defaults, override)\n",
    "    \n",
    "    @staticmethod\n",
    "    def process(*args, **kwargs):\n",
    "        processor = Preprocessor(*args, **kwargs)\n",
    "        processor.apply()\n",
    "        return processor\n",
    "    \n",
    "    def __init__(self, data, test_index=None, train_index=None, options=None):\n",
    "        if options is None:\n",
    "            options = Preprocessor.params()\n",
    "        \n",
    "        if train_index is None:\n",
    "            train_index = list(range(len(data)))\n",
    "            test_index = []\n",
    "        \n",
    "        self.data = data\n",
    "        self.test_index = np.array(test_index)\n",
    "        self.train_index = np.array(train_index)\n",
    "        self.options = options\n",
    "        \n",
    "        self.target = self.options['target']\n",
    "        self.update_meta()\n",
    "    \n",
    "    def update_meta(self):\n",
    "        self.features_mask = self.data.columns != self.target\n",
    "        self.columns = self.data.columns\n",
    "        self.feature_columns = self.columns[self.features_mask]\n",
    "        self.has_train = self.train_index.shape[0] > 0\n",
    "        self.has_test = self.test_index.shape[0] > 0\n",
    "\n",
    "    def override_data(self, value):\n",
    "        labels = self.y\n",
    "        self.data = pd.DataFrame(data=value, index=self.data.index)\n",
    "        self.data[self.target] = labels\n",
    "        self.update_meta()\n",
    "            \n",
    "    def get_x(self, df):\n",
    "        return df.drop(self.target, axis=1).to_numpy()\n",
    "    \n",
    "    def get_y(self, df):\n",
    "        return df[self.target].to_numpy()\n",
    "    \n",
    "    # .x getter/setter\n",
    "    @property\n",
    "    def x(self):\n",
    "        return self.get_x(self.data)\n",
    "    \n",
    "    @x.setter\n",
    "    def x(self, value):\n",
    "        self.data.loc[:, self.features_mask] = value\n",
    "        \n",
    "    # .y getter/setter\n",
    "    @property\n",
    "    def y(self):\n",
    "        return self.get_y(self.data)\n",
    "    \n",
    "    @y.setter\n",
    "    def y(self, value):\n",
    "        self.data.loc[:, [self.target]] = value\n",
    "    \n",
    "    # .test getter/setter\n",
    "    @property\n",
    "    def test(self):\n",
    "        return self.data.loc[self.test_index]\n",
    "    \n",
    "    @test.setter\n",
    "    def test(self, value):\n",
    "        self.data.loc[self.test_index] = value\n",
    "    \n",
    "    # .train getter/setter\n",
    "    @property\n",
    "    def train(self):\n",
    "        return self.data.loc[self.train_index]\n",
    "    \n",
    "    @train.setter\n",
    "    def train(self, value):\n",
    "        self.data.loc[self.train_index] = value\n",
    "    \n",
    "    # .x_test getter/setter\n",
    "    @property\n",
    "    def x_test(self):\n",
    "        return self.get_x(self.test)\n",
    "    \n",
    "    @x_test.setter\n",
    "    def x_test(self, value):\n",
    "        self.data.loc[self.test_index, self.features_mask] = value\n",
    "        \n",
    "    # .x_train getter/setter\n",
    "    @property\n",
    "    def x_train(self):\n",
    "        return self.get_x(self.train)\n",
    "    \n",
    "    @x_train.setter\n",
    "    def x_train(self, value):\n",
    "        self.data.loc[self.train_index, self.features_mask] = value\n",
    "    \n",
    "    # .y_test getter\n",
    "    @property\n",
    "    def y_test(self):\n",
    "        return self.get_y(self.test)\n",
    "        \n",
    "    # .y_test setter\n",
    "    @property\n",
    "    def y_train(self):\n",
    "        return self.get_y(self.train)\n",
    "    \n",
    "    def chop(self):\n",
    "        return self.x_test, self.y_test, self.x_train, self.y_train\n",
    "    \n",
    "    def apply(self):\n",
    "        # remove outliers\n",
    "        outlier_strategy = self.options.get('outlier_strategy', 'train_only')\n",
    "        outlier_remover = OutlierRemover.numeric(self.data)\n",
    "        if outlier_strategy == 'train_only':\n",
    "            self.train = outlier_remover.fit_transform(self.train)\n",
    "        elif outlier_strategy == 'include_test':\n",
    "            outlier_remover.fit(self.train())\n",
    "            self.data = outlier_remover.transform(self.data)\n",
    "        elif outlier_strategy == 'all':\n",
    "            self.data = outlier_remover.fit_transform(self.data)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        # update removed indexes.\n",
    "        indexes = self.data.index.values\n",
    "        self.train_index = self.train_index[np.isin(self.train_index, indexes)]\n",
    "        self.test_index = self.test_index[np.isin(self.test_index, indexes)]\n",
    "        \n",
    "        # encode labels\n",
    "        encode_labels = self.options.get('encode_labels', True)\n",
    "        if encode_labels:\n",
    "            self.data = MultiLabelEncoder.binary(self.data).fit_transform(self.data)\n",
    "            \n",
    "        onehot_encoding = self.options.get('onehot_encoding', None)\n",
    "        if onehot_encoding is not None:\n",
    "            cols = onehot_encoding\n",
    "            self.data = pd.get_dummies(self.data, columns=cols, prefix=cols)\n",
    "            self.update_meta()\n",
    "            \n",
    "        # ordinal encoding\n",
    "        ordinal_encoding = self.options.get('ordinal_encoding', None)\n",
    "        if ordinal_encoding is not None:\n",
    "            for col, ordinals in ordinal_encoding.items():\n",
    "                encoder = OrdinalEncoder(categories=[ordinals])\n",
    "                self.data[[col]] = encoder.fit_transform(self.data[[col]])\n",
    "        \n",
    "        # scaler\n",
    "        scale = self.options.get('scale', True)\n",
    "        if scale:\n",
    "            scaler = StandardScaler()\n",
    "            self.x_train = scaler.fit_transform(self.x_train)\n",
    "            \n",
    "            if self.has_test:\n",
    "                self.x_test = scaler.transform(self.x_test)\n",
    "                \n",
    "        # PCA\n",
    "        pca_n = self.options.get('pca', False)\n",
    "        if pca_n:\n",
    "            pca = PCA(n_components=pca_n, svd_solver='full', copy=True)\n",
    "            pca.fit(self.x_train)\n",
    "            result = pca.transform(self.x)\n",
    "            self.override_data(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa74c236-cab3-4c93-93b8-222d7650a27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, estimator, data, target, scoring, n_splits, test_size, seed,\n",
    "                 prep_params=None, hp_grid=None):\n",
    "        \n",
    "        self.estimator = estimator\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.scoring = scoring\n",
    "        self.n_splits = n_splits\n",
    "        self.test_size = test_size\n",
    "        self.seed = seed\n",
    "        self.prep_params = prep_params\n",
    "        self.hp_grid = hp_grid\n",
    "\n",
    "        self.results = []\n",
    "        self.best_result = None\n",
    "        \n",
    "        self.parse_args()\n",
    "    \n",
    "    def parse_args(self):\n",
    "        self.n_test_split, self.n_grid_split = self.n_splits\n",
    "        self.score_type, self.score_fn = self.scoring\n",
    "    \n",
    "    @property\n",
    "    def model(self):\n",
    "        return self.best_result['model']\n",
    "    \n",
    "    @property\n",
    "    def preprocessor(self):\n",
    "        return self.best_result['preprocessor']\n",
    "    \n",
    "    @property\n",
    "    def scores(self):\n",
    "        return self.best_result['scores']\n",
    "    \n",
    "    @property\n",
    "    def score(self):\n",
    "        return self.scores[self.score_type][self.score_fn]\n",
    "    \n",
    "    def split(self):\n",
    "        split = StratifiedShuffleSplit(n_splits=self.n_test_split, test_size=self.test_size, random_state=self.seed)\n",
    "        return split.split(self.data, self.data[self.target])\n",
    "    \n",
    "    def train(self, tick=None):\n",
    "        for split_index, (train_index, test_index) in enumerate(self.split()):\n",
    "            if tick is not None:\n",
    "                tick(split_index/self.n_test_split)\n",
    "            \n",
    "            # default values\n",
    "            best_params = None\n",
    "            best_estimator = None\n",
    "            \n",
    "            # preprocess data\n",
    "            preprocessor = Preprocessor.process(self.data, test_index, train_index, self.prep_params)\n",
    "            X_test, Y_test, X_train, Y_train = preprocessor.chop()\n",
    "                        \n",
    "            # fix grid\n",
    "            is_grid, hp_grid = self.fix_hp_grid(self.hp_grid)\n",
    "            \n",
    "            # if given parameters are grid do grid search\n",
    "            if is_grid:\n",
    "                # create grid searcher\n",
    "                gscv = GridSearchCV(estimator=self.estimator(), param_grid=hp_grid,\n",
    "                                  cv=self.n_grid_split, scoring=self.score_fn, n_jobs=-1)\n",
    "                \n",
    "                # fit\n",
    "                gscv.fit(X_train, Y_train)\n",
    "                \n",
    "                # collect best results\n",
    "                best_params = gscv.best_params_\n",
    "                best_estimator = gscv.best_estimator_\n",
    "            \n",
    "            # if given parameters are singular or none do direct training.\n",
    "            else:\n",
    "                # create and fit estimator\n",
    "                best_estimator = self.estimator(**hp_grid)\n",
    "                best_estimator.fit(X_train, Y_train)\n",
    "                best_params = hp_grid\n",
    "            \n",
    "            # get predictions\n",
    "            Y_pred = best_estimator.predict(X_test)\n",
    "            Y_prob = best_estimator.predict_proba(X_test)\n",
    "            \n",
    "            # create result \n",
    "            self.results.append({\n",
    "                'y_true': Y_test, 'y_pred': Y_pred, 'y_prob': Y_prob, \n",
    "                'params': best_params, 'model': best_estimator,\n",
    "                'preprocessor': preprocessor, 'seed': self.seed,\n",
    "            })\n",
    "            \n",
    "        self._calculate_scores()\n",
    "        self._set_best_result()\n",
    "    \n",
    "    def fix_hp_grid(self, hp_grid=None):\n",
    "        if hp_grid is None:\n",
    "            return False, {}\n",
    "\n",
    "        # check if there is multidimensional value.\n",
    "        is_grid = sum([np.ndim(v) for v in hp_grid.values()]) > 0\n",
    "        \n",
    "        # fix singular values if suppose to be a grid.\n",
    "        if is_grid:\n",
    "            hp_grid = {k: [v] if np.ndim(v) == 0 else v for k, v in hp_grid.items()}\n",
    "\n",
    "        return is_grid, hp_grid\n",
    "    \n",
    "    def move_threshold(self, prob, threshold):\n",
    "        return (prob[:, 1] >= t).astype(int)\n",
    "    \n",
    "    def _calculate_scores(self):\n",
    "        score_fns = {\n",
    "            'accuracy': accuracy_score,\n",
    "            'f1': f1_score,\n",
    "            'recall': recall_score,\n",
    "            'precision': precision_score,\n",
    "        }\n",
    "        \n",
    "        # calculate for each result.\n",
    "        for result in self.results:\n",
    "            # get targets\n",
    "            Y_true, Y_pred, Y_prob = result['y_true'], result['y_pred'], result['y_prob']\n",
    "            \n",
    "            # create scores\n",
    "            scores = result['scores'] = dict(raw={}, moved={})\n",
    "            \n",
    "            # calculate raw scores\n",
    "            for name, fn in score_fns.items():\n",
    "                scores['raw'][name] = fn(Y_true, Y_pred)\n",
    "            \n",
    "            # calculate threshold moved scores\n",
    "            moved_score = -1\n",
    "            moved_pred = None\n",
    "            moved_threshold = None\n",
    "            \n",
    "            # find the best threshold\n",
    "            for threshold in np.arange(0.5, 0, -0.01):\n",
    "                pred = (Y_prob[:, 1] >= threshold).astype(int)\n",
    "                score = score_fns[self.score_fn](Y_true, pred)\n",
    "                \n",
    "                if score > moved_score:\n",
    "                    print('got')\n",
    "                    moved_pred = pred\n",
    "                    moved_score = score\n",
    "                    moved_threshold = threshold\n",
    "            \n",
    "            # calculate threshold moved scores\n",
    "            for name, fn in score_fns.items():\n",
    "                scores['moved'][name] = fn(Y_true, moved_pred)\n",
    "            \n",
    "            # keep the threshold info\n",
    "            result['moved_threshold'] = moved_threshold\n",
    "    \n",
    "    def _set_best_result(self):\n",
    "        total_score = 0\n",
    "        best_score = -1\n",
    "        best_result = None\n",
    "        \n",
    "        for result in self.results:\n",
    "            score = result['scores'][self.score_type][self.score_fn]\n",
    "\n",
    "            total_score += score\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_result = result\n",
    "        \n",
    "        self.best_result = best_result\n",
    "        self.score_mean = total_score / len(self.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9afae4-9888-4332-b759-b7a057025508",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Trainer:   \n",
    "    def __init__(self, name, data, target, scoring, n_splits, test_size, seed, prep_params=None):\n",
    "        self.name = name\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.scoring = scoring\n",
    "        self.n_splits = n_splits\n",
    "        self.test_size = test_size\n",
    "        self.seed = seed\n",
    "        self.prep_params = prep_params\n",
    "        self.estimators = {}\n",
    "    \n",
    "    def add_estimator(self, name, estimator, hp_grid=None):\n",
    "        self.estimators[name] = (name, estimator, hp_grid)\n",
    "    \n",
    "    def update_estimator_params(self, name, hp_grid):\n",
    "        name, estimator, _ = self.estimators[name]\n",
    "        self.estimators[name] = (name, estimator, hp_grid)\n",
    "        \n",
    "    def apply_estimator_params(self, params):\n",
    "        for name, hp_grid in params.items():\n",
    "            self.update_estimator_params(name, hp_grid)\n",
    "    \n",
    "    def get_model_path(self, name):\n",
    "        return path.join(path_models, f'{self.name}_{name}.pickle')\n",
    "    \n",
    "    def save_model(self, name, model):\n",
    "        model_path = self.get_model_path(name)\n",
    "        with open(model_path,'wb') as file:\n",
    "            pickle.dump(model, file)\n",
    "        \n",
    "    def load_model(self, name):\n",
    "        model_path = self.get_model_path(name)\n",
    "        with open(model_path, 'rb') as file:\n",
    "            return pickle.load(file)\n",
    "        \n",
    "    def train_estimators(self, **kwargs):\n",
    "        estimators = kwargs.pop('estimators', self.estimators.keys())\n",
    "        for name in estimators:\n",
    "            print_red(f'Estimator: {name}\\n')\n",
    "            model = self.train_estimator(name, **kwargs)\n",
    "            yield (name, model)\n",
    "            \n",
    "    def train_estimator(self, name, reset=False, seed=None, save=True, print_duration=True):      \n",
    "        if seed is None:\n",
    "            seed = self.seed\n",
    "        \n",
    "        if not reset:\n",
    "            try:\n",
    "                model = self.load_model(name)\n",
    "                setattr(self, name, model)\n",
    "                \n",
    "                print(f'Model {name} is loaded from disk successfully.')\n",
    "                return model\n",
    "            \n",
    "            except:\n",
    "                model = None\n",
    "        \n",
    "        name, estimator, hp_grid = self.estimators[name]\n",
    "        \n",
    "        model = Model(estimator, self.data, self.target, self.scoring, self.n_splits,\n",
    "                    self.test_size, seed, self.prep_params, hp_grid)\n",
    "        \n",
    "        if print_duration:\n",
    "            with PrintDuration() as tick:\n",
    "                model.train(tick)\n",
    "        else:\n",
    "            model.train()\n",
    "        \n",
    "        setattr(self, name, model)\n",
    "        if save:\n",
    "            self.save_model(name, model)\n",
    "            \n",
    "        return model\n",
    "    \n",
    "    def search_best_seed(self, name, seed_range=range(100)):\n",
    "        output = Output()\n",
    "        \n",
    "        best_score = -1\n",
    "        best_seed = 0\n",
    "        \n",
    "        print(f'Searching best seed for {name}')\n",
    "        \n",
    "        for seed in seed_range:\n",
    "            output.update(f'  -> Testing seed {seed}')\n",
    "            model = self.train_estimator(name=name, seed=seed, save=False, print_duration=False)\n",
    "            score = model.score\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_seed = seed\n",
    "                print(f'* {seed} -> {score} - {model.score_mean}')\n",
    "        \n",
    "        print(f'Best seed found as {best_seed}')\n",
    "        return best_seed\n",
    "\n",
    "    def get_results_df(self, name, shuffle=False, ascending=False):\n",
    "        model = getattr(self, name)\n",
    "\n",
    "        true = model.best_result['y_true'].reshape(-1)\n",
    "        pred = model.best_result['y_pred'].reshape(-1)\n",
    "        \n",
    "        df = pd.DataFrame(data={\n",
    "            'true': true,\n",
    "            'prediction': pred,\n",
    "            'diff': np.absolute(true - pred)\n",
    "        })\n",
    "    \n",
    "        if shuffle:\n",
    "            df = df.sample(frac=1)\n",
    "        else:\n",
    "            df = df.sort_values('diff', ascending=ascending)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_scores_df(self, name):\n",
    "        model = getattr(self, name)\n",
    "        stats = model.best_stats\n",
    "\n",
    "        # collect stats\n",
    "        cols = ['accuracy', 'f1', 'recall', 'precision']\n",
    "        values = [[stats[col] for col in cols]]\n",
    "        index = [name]\n",
    "\n",
    "        # additional stats\n",
    "        cols += [f'mean {self.scoring}']\n",
    "        values[0] += [model.mean_score]\n",
    "\n",
    "        return pd.DataFrame(values, index, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad29e17-8d15-479f-ac58-f0280a4dc83f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SetTrainer:\n",
    "    def __init__(self):\n",
    "        self.estimators = {}\n",
    "        self.trainer_names = []\n",
    "        \n",
    "    def add_estimator(self, name, estimator, hp_grid=None):\n",
    "        self.estimators[name] = (name, estimator, hp_grid)\n",
    "    \n",
    "    def add_trainer(self, **kwargs):\n",
    "        name = kwargs['name']\n",
    "        trainer = Trainer(**kwargs)\n",
    "        \n",
    "        for estimator_name in self.estimators:\n",
    "            _, estimator, hp_grid = self.estimators[estimator_name]\n",
    "            hp_grid = deepcopy(hp_grid)\n",
    "            trainer.add_estimator(estimator_name, estimator, hp_grid)\n",
    "        \n",
    "        self.trainer_names.append(name)\n",
    "        setattr(self, name, trainer)\n",
    "        \n",
    "    def run_trainer(self, name, **kwargs):\n",
    "        trainer = getattr(self, name)\n",
    "        for (model_name, model) in trainer.train_estimators(**kwargs):\n",
    "            yield (name, trainer, model_name, model)\n",
    "            \n",
    "    def run_all_trainers(self, **kwargs):\n",
    "        trainers = kwargs.pop('trainers', self.trainer_names)\n",
    "        count = len(trainers)\n",
    "        \n",
    "        for index, name in enumerate(trainers):\n",
    "            print_blue(f'Trainer {index+1}/{count}: {name}\\n')\n",
    "            for (trainer_name, trainer, model_name, model) in self.run_trainer(name, **kwargs):\n",
    "                yield (trainer_name, trainer, model_name, model)\n",
    "    \n",
    "set_trainer = SetTrainer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c033179c-4f7a-449a-a301-5301cbe94531",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cac4266-3809-4fc6-9b64-b12e26d554c9",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e8d6c5-940b-432e-9f23-3fce89762671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_feature_importances(data, target, scoring, n_splits, test_size, seed, prep_params=None):\n",
    "    model = Model(RandomForestClassifier, data, target, scoring, n_splits, test_size, seed, prep_params)\n",
    "    model.train()\n",
    "    \n",
    "    importances = model.model.feature_importances_\n",
    "    indices = np.argsort(importances)\n",
    "    graph_x = range(len(indices))\n",
    "    \n",
    "    plt.title('Feature Importances')\n",
    "    plt.barh(graph_x, importances[indices], color='b', align='center')\n",
    "    plt.yticks(graph_x, model.preprocessor.feature_columns[indices])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.show()\n",
    "\n",
    "data_ = use_limited_data(full_data, 3200)\n",
    "calculate_feature_importances(data=data_, target=TARGET_COLUMN, scoring=hp_scoring,\n",
    "                              n_splits=hp_cv_splits, test_size=hp_test_size, seed=hp_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4e4b8d-e880-48f1-85c8-c1c0b59ce133",
   "metadata": {},
   "source": [
    "### Explained Variance Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4f055e-c2b1-456b-9022-c1aea58d570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_explained_variance_ratio(data, p=0.95):\n",
    "    X = Preprocessor.process(data).data\n",
    "    \n",
    "    pca = PCA(n_components=None, svd_solver='full', copy=True)\n",
    "    reduced = pca.fit_transform(X)\n",
    "    \n",
    "    # extract the smallest number of components which\n",
    "    # explain at least p% (e.g. 80%) of the variance\n",
    "    n_components = 1 + np.argmax(np.cumsum(pca.explained_variance_ratio_) >= p)\n",
    "    print(f'For p={int(p*100)}% n_components should be {n_components}\\n')\n",
    "\n",
    "    # extract the values of the selected components\n",
    "    #Z = pca.transform(X)[:, :n_components]\n",
    "    \n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('number of components')\n",
    "    plt.ylabel('cumulative explained variance')\n",
    "    \n",
    "data_ = use_limited_data(full_data, 3200)\n",
    "calculate_explained_variance_ratio(data_, 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc31471-b278-48ad-8492-19f6d513fcb0",
   "metadata": {},
   "source": [
    "### Best Seed Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305f03ba-fdbd-45c9-a92d-3034cd1b151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_best_seed(seed_range, estimators, data, target, scoring, n_splits, test_size, prep_params=None):\n",
    "    trainer = Trainer('trainer', data, target, scoring, n_splits, test_size,\n",
    "                      seed=0, prep_params=prep_params)\n",
    "\n",
    "    for estimator in estimators:\n",
    "        name = estimator.__name__\n",
    "        \n",
    "        trainer.add_estimator(name, estimator)\n",
    "        trainer.search_best_seed(name, seed_range)\n",
    "\n",
    "calculate_best_seed(seed_range=range(0), estimators=[LogisticRegression],\n",
    "                    data=full_data, target=TARGET_COLUMN, scoring=hp_scoring,\n",
    "                    n_splits=(3, 1), test_size=hp_test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6438daf-1ae4-42b0-8371-e7d412edb454",
   "metadata": {},
   "source": [
    "# Model Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844a9826-8b4f-4edc-b4da-c4d1f0111faa",
   "metadata": {},
   "source": [
    "## Logistic Regression \n",
    "[docs](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a942b4f2-3471-47b6-87f1-fff0c413676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add estimator\n",
    "set_trainer.add_estimator('logistic', LogisticRegression, {\n",
    "    'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "    'C': [0.2, 0.5, 0.8],\n",
    "    'solver': ['saga'],\n",
    "    'max_iter': [10, 20, 40],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09531e2-69a2-4af0-80d3-17b2a7ae4e82",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes \n",
    "[docs](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html), [comparing naive bayes classification algorithms](https://towardsdatascience.com/comparing-a-variety-of-naive-bayes-classification-algorithms-fc5fa298379e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d378294b-f0d9-4d0b-90a2-1cc937165625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add estimator\n",
    "set_trainer.add_estimator('nb_gaussian', GaussianNB, {\n",
    "    'var_smoothing': [0, 1e-10, 1e-9, 1e-8]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aec9e2-11a5-4f6d-80de-3021230c6390",
   "metadata": {},
   "source": [
    "## kNN \n",
    "[docs](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a75ea74-2684-47b1-ad08-4e177ff9a31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add estimator\n",
    "set_trainer.add_estimator('knn', KNeighborsClassifier, {\n",
    "    'n_neighbors': [1, 2, 3, 4],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'p': [1, 2],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bafe1b-c286-4577-99d6-12af68ca7d3d",
   "metadata": {},
   "source": [
    "## SVM \n",
    "[docs](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8879e006-8835-4eb1-8b59-c2ba6b528eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add estimator\n",
    "set_trainer.add_estimator('svm', SVC, {\n",
    "    'kernel': ['linear', 'poly', 'rbf'],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'C': [10, 12, 15],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8242ecbe-45b5-4b4e-8f69-ba3479fcaf81",
   "metadata": {},
   "source": [
    "## Decision Tree \n",
    "[docs](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b449782-e490-4fb9-99b9-067e775eecfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add estimator\n",
    "set_trainer.add_estimator('dt', DecisionTreeClassifier, {\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'min_samples_split': [2, 4, 8],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'max_features': [1, 0.8, 0.5],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbc7161-56a1-41ac-b392-6237082b3787",
   "metadata": {},
   "source": [
    "## MLP \n",
    "[docs](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b3f470-be66-4230-92b7-c9f69ee88be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add estimator\n",
    "set_trainer.add_estimator('mlp', MLPClassifier, {\n",
    "    'hidden_layer_sizes': [(100,), (128,128), (256,)],\n",
    "    'activation': ['relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    'learning_rate_init': [0.001],\n",
    "    'early_stopping': [True],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435288b3-f29b-437f-a930-03fb3c6d8ca8",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "[docs](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6add1530-4f25-4ab6-b149-0f331bc74f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add estimator\n",
    "set_trainer.add_estimator('bag', BaggingClassifier, {\n",
    "    'n_estimators': [10, 20, 50],\n",
    "    'max_samples': [1, 0.8, 0.5],\n",
    "    'max_features': [1, 0.8, 0.5],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dafccce-a01a-451f-81f1-69a5c45cbb63",
   "metadata": {},
   "source": [
    "## Random Forest \n",
    "[docs](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f14bec0-d396-42b0-99e8-3f45b0fbad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add estimator\n",
    "set_trainer.add_estimator('rf', RandomForestClassifier, {\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'min_samples_split': [2, 4],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': [1, 0.7],\n",
    "    'max_samples': [1, 0.7],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f6f809-7175-47ab-b085-abfa645b5455",
   "metadata": {},
   "source": [
    "## Gradient Boosting \n",
    "[docs](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0166a4d8-5182-4da3-a029-1e13115b106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add estimator\n",
    "set_trainer.add_estimator('gb', GradientBoostingClassifier, {\n",
    "    'min_samples_split': [2, 4],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_depth': [3, 5],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e1ef86-ceb8-48b4-8579-3baa672563ad",
   "metadata": {},
   "source": [
    "## AdaBoost \n",
    "[docs](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c54d9a5-7d34-4b75-b44d-751d67a8562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add estimator\n",
    "set_trainer.add_estimator('ada', LGBMClassifier, {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.5, 1.0, 2.0],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16836017-270d-4598-8779-a8d6fcc3dcb6",
   "metadata": {},
   "source": [
    "## LightGBM\n",
    "[docs](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c6722b-df47-45e6-a9a5-202b1b37f58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add estimator\n",
    "set_trainer.add_estimator('lgbm', LGBMClassifier, {\n",
    "    'boosting_type': ['gbdt', 'dart', 'goss', 'rf'],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'objective': ['binary'],\n",
    "    'subsample': [0.5, 0.75, 1],\n",
    "    'colsample_bytree': [0.5, 0.75, 1],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b2eca7-3163-46d5-9699-285b6e4a11be",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Name \n",
    "[docs]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4100e5b2-fcb5-4165-b525-008fe573ebaf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36468257-ab77-4496-9b25-da7630dc57eb",
   "metadata": {},
   "source": [
    "### Default Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774b04ba-7138-4a79-8be0-db54c94c45e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_params = Preprocessor.params({\n",
    "    'outlier_strategy': 'all',\n",
    "    'encode_labels': True,\n",
    "    'pca': False,\n",
    "})\n",
    "\n",
    "set_trainer.add_trainer(name='default', data=data, target=TARGET_COLUMN, scoring=hp_scoring,\n",
    "        n_splits=hp_cv_splits, test_size=hp_test_size, seed=hp_seed, prep_params=prep_params)\n",
    "\n",
    "# True = Single parameters, no grid search.\n",
    "# False = Default grid parameters used, will do grid search.\n",
    "if True:\n",
    "    set_trainer.default.apply_estimator_params({\n",
    "        'logistic': {'C': 0.2, 'max_iter': 10, 'penalty': 'l2', 'solver': 'saga'},\n",
    "    })\n",
    "    \n",
    "    # Gaussian Naive Bayes\n",
    "    set_trainer.default.update_estimator_params('nb_gaussian', {'var_smoothing': 1e-08})\n",
    "    \n",
    "    # kNN\n",
    "    set_trainer.default.update_estimator_params('knn', {'n_neighbors': 1, 'p': 2, 'weights': 'uniform'})\n",
    "    \n",
    "    # SVM\n",
    "    set_trainer.default.update_estimator_params('svm', {'C': 12, 'gamma': 'scale', 'kernel': 'poly'})\n",
    "    \n",
    "    # Decision Trees\n",
    "    set_trainer.default.update_estimator_params('dt', {\n",
    "        'criterion': 'gini', 'max_features': 0.8, 'min_samples_leaf': 3,\n",
    "        'min_samples_split': 4, 'splitter': 'best'\n",
    "    })\n",
    "    \n",
    "    # MLP\n",
    "    set_trainer.default.update_estimator_params('mlp', {\n",
    "        'activation': 'relu', 'early_stopping': True, 'hidden_layer_sizes': (256,),\n",
    "        'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'sgd'\n",
    "    })\n",
    "    \n",
    "    # Bagging\n",
    "    set_trainer.default.update_estimator_params('bag', {\n",
    "        'max_features': 0.8, 'max_samples': 0.5, 'n_estimators': 20\n",
    "    })\n",
    "    \n",
    "    # Random Forests\n",
    "    set_trainer.default.update_estimator_params('rf', {\n",
    "        'criterion': 'gini', 'max_features': 0.7, 'max_samples': 0.7,\n",
    "        'min_samples_leaf': 1, 'min_samples_split': 4\n",
    "    })\n",
    "    \n",
    "    # Gradient Boosting\n",
    "    set_trainer.default.update_estimator_params('gb', {\n",
    "        'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2\n",
    "    })\n",
    "    \n",
    "    # AdaBoost\n",
    "    set_trainer.default.update_estimator_params('ada', {'learning_rate': 1.0, 'n_estimators': 100})\n",
    "    \n",
    "    # LGBM\n",
    "    set_trainer.default.update_estimator_params('lgbm', {\n",
    "        'boosting_type': 'goss', 'colsample_bytree': 0.75, 'learning_rate': 0.2,\n",
    "        'n_estimators': 100, 'objective': 'binary', 'subsample': 0.5\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d22dce5-7bc9-47ad-bdaf-0be55a99a9af",
   "metadata": {},
   "source": [
    "### PCA Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6788605-9a75-432a-ae71-ad47cac98d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_params = Preprocessor.params({\n",
    "    'outlier_strategy': 'all',\n",
    "    'encode_labels': True,\n",
    "    'pca': 0.95,\n",
    "})\n",
    "\n",
    "set_trainer.add_trainer(name='pca', data=data, target=TARGET_COLUMN, scoring=hp_scoring,\n",
    "        n_splits=hp_cv_splits, test_size=hp_test_size, seed=hp_seed, prep_params=prep_params)\n",
    "\n",
    "# True = Single parameters, no grid search.\n",
    "# False = Default grid parameters used, will do grid search.\n",
    "if True:\n",
    "    set_trainer.default.apply_estimator_params({\n",
    "        'logistic': {'C': 0.5, 'max_iter': 10, 'penalty': 'l2', 'solver': 'saga'},\n",
    "    })\n",
    "        \n",
    "    # Gaussian Naive Bayes\n",
    "    set_trainer.pca.update_estimator_params('nb_gaussian', {'var_smoothing': 0})\n",
    "    \n",
    "    # kNN\n",
    "    set_trainer.pca.update_estimator_params('knn', {'n_neighbors': 1, 'p': 2, 'weights': 'uniform'})\n",
    "    \n",
    "    # SVM\n",
    "    set_trainer.pca.update_estimator_params('svm', {'C': 12, 'gamma': 'scale', 'kernel': 'poly'})\n",
    "    \n",
    "    # Decision Trees\n",
    "    set_trainer.pca.update_estimator_params('dt', {\n",
    "        'criterion': 'entropy', 'max_features': 0.5, 'min_samples_leaf': 2,\n",
    "        'min_samples_split': 4, 'splitter': 'best'\n",
    "    })\n",
    "    \n",
    "    # MLP\n",
    "    set_trainer.pca.update_estimator_params('mlp', {\n",
    "        'activation': 'relu', 'early_stopping': True, 'hidden_layer_sizes': (256,),\n",
    "        'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'sgd'\n",
    "    })\n",
    "    \n",
    "    # Bagging\n",
    "    set_trainer.pca.update_estimator_params('bag', {\n",
    "        'max_features': 0.8, 'max_samples': 0.5, 'n_estimators': 50\n",
    "    })\n",
    "    \n",
    "    # Random Forests\n",
    "    set_trainer.pca.update_estimator_params('rf', {\n",
    "        'criterion': 'gini', 'max_features': 0.7, 'max_samples': 0.7,\n",
    "        'min_samples_leaf': 1, 'min_samples_split': 2\n",
    "    })\n",
    "    \n",
    "    # Gradient Boosting\n",
    "    set_trainer.pca.update_estimator_params('gb', {\n",
    "        'max_depth': 3, 'min_samples_leaf': 2, 'min_samples_split': 2\n",
    "    })\n",
    "    \n",
    "    # AdaBoost\n",
    "    set_trainer.pca.update_estimator_params('ada', {'learning_rate': 0.5, 'n_estimators': 200})\n",
    "    \n",
    "    # LGBM\n",
    "    set_trainer.pca.update_estimator_params('lgbm', {\n",
    "        'boosting_type': 'dart', 'colsample_bytree': 1, 'learning_rate': 0.1,\n",
    "        'n_estimators': 200, 'objective': 'binary', 'subsample': 0.5\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3779f6bb-495d-4274-87bb-44d4b560ef0c",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5c76bb-9150-422b-9870-1ea3d3c81f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    # set trainer parameters\n",
    "    'trainers': ['default'],\n",
    "    \n",
    "    # trainer parameters\n",
    "    'estimators': ['logistic'],\n",
    "    #'reset': False,\n",
    "    #'seed': None,\n",
    "    'save': False,\n",
    "}\n",
    "\n",
    "for (trainer_name, trainer, model_name, model) in set_trainer.run_all_trainers(**options):\n",
    "    # Get best params.\n",
    "    print_dim(f\"\\nbest params: {model.best_result['params']}\")\n",
    "    \n",
    "    # Show stats.\n",
    "    print()\n",
    "    #trainer.get_scores_df(model_name).head()\n",
    "    print()\n",
    "\n",
    "    # Show predicts.\n",
    "    #trainer.get_results_df(model_name, ascending=True).head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
